{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c6dfe6-5908-4cf7-a562-95771f8b9227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to francesca.restante\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Application Id:spark-b164bd43055d4fb48c7f20a935507146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = 73440d65-d0e7-40df-bd0b-0d3001b4a0ba\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|           namespace|\n",
      "+--------------------+\n",
      "|              app_io|\n",
      "|business_intellig...|\n",
      "|            cashback|\n",
      "|           checkiban|\n",
      "|      data_engineers|\n",
      "|       data_products|\n",
      "|   data_products_dev|\n",
      "|     data_scientists|\n",
      "|       data_strategy|\n",
      "|             default|\n",
      "|      dl_anagrafiche|\n",
      "|       dl_tassonomie|\n",
      "|             dtd_bul|\n",
      "|         dtd_bul_dev|\n",
      "|             dtd_dev|\n",
      "|             dtd_fse|\n",
      "|         dtd_fse_dev|\n",
      "|           dtd_istat|\n",
      "|           dtd_pad26|\n",
      "|       dtd_pad26_dev|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cml.data_v1 as cmldata\n",
    "\n",
    "import configparser\n",
    "import uuid\n",
    "import os\n",
    "from typing import Dict\n",
    "from pyspark.sql.functions import to_date, col\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession, DataFrameWriter\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, DateType, StringType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta, date\n",
    "from pyspark.sql.functions import col, ceil, when\n",
    "import pandas as pd\n",
    "\n",
    "# Sample in-code customization of spark configurations\n",
    "#from pyspark import SparkContext\n",
    "#SparkContext.setSystemProperty('spark.executor.cores', '1')\n",
    "#SparkContext.setSystemProperty('spark.executor.memory', '2g')\n",
    "\n",
    "CONNECTION_NAME = \"pdnd-prod-dl-1\"\n",
    "conn = cmldata.get_connection(CONNECTION_NAME)\n",
    "spark = conn.get_spark_session()\n",
    "\n",
    "# Sample usage to run query through spark\n",
    "EXAMPLE_SQL_QUERY = \"show databases\"\n",
    "spark.sql(EXAMPLE_SQL_QUERY).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0035c45c-6a7a-47eb-a416-40dd4ffb0024",
   "metadata": {},
   "source": [
    "### Query iniziale filtrata \n",
    "##### prendo solamente i dati degli ultimi 3 mesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d24b34ab-8d33-469a-a33b-82f9f869a806",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold = spark.sql( \"\"\"   \n",
    "                       SELECT  requestid,\n",
    "                               iun,\n",
    "                               geokey,\n",
    "                               accettazione_recapitista_con018_data\n",
    "                       FROM send.gold_postalizzazione_analytics\n",
    "                       WHERE accettazione_recapitista_con018_data IS NOT NULL\n",
    "                       AND senderpaid = \"53b40136-65f2-424b-acfb-7fae17e35c60\" \n",
    "                       AND requesttimestamp >= add_months(current_timestamp(), -3)\n",
    "                       AND requestid NOT IN (SELECT requestid_computed\n",
    "                                              FROM send.silver_postalizzazione_denormalized\n",
    "                                              WHERE statusrequest IN ('PN999', 'PN998'))\"\"\"   \n",
    "                    ) "
   ]
  },
  {
   "cell_type": "raw",
   "id": "050b7278-6b93-469f-8ec6-216bd2d95348",
   "metadata": {},
   "source": [
    "df_gold.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74c61d8e-4631-4544-ac1d-7af2617cd66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gold.createOrReplaceTempView(\"DF_GOLD\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "94627da8-45d7-4726-9d7a-3ad6ed217cef",
   "metadata": {},
   "source": [
    "print(df_gold.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae70b24c-5239-44bb-9423-008958edd644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definire lo schema per il nuovo df di spark che comprende i lotti e regioni associati \n",
    "schema = StructType([\n",
    "    StructField(\"CAP\", StringType(), True), \n",
    "    StructField(\"Regione\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ec93b51-e578-407c-821d-d40b8b41a222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap_regione = spark.read.csv(\"CAP-Regione.csv\", header= True, sep= \";\", schema = schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c3cf551-0a5a-4309-b68f-39344538bb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap_regione = df_cap_regione.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e3e73a9a-a208-4918-aff1-2929dec05c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "4691"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cap_regione.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b6616d5-581b-41b7-b323-77c75d66093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap_regione.createOrReplaceTempView(\"DF_CAP_REGIONE\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfa65009-51ac-47af-ab8c-a4c6caff3d1f",
   "metadata": {},
   "source": [
    "print(df_cap_regione)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3bbf2c-bb87-43b2-b816-208b76013f62",
   "metadata": {},
   "source": [
    "# Calcolo delle numeriche per regioni e numeriche totali di affidi con con018 <> null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6464d12-330f-4fa6-9609-e7b50a2eeb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(\"\"\"SELECT r.Regione,\n",
    "                    MONTH(g.accettazione_recapitista_con018_data) AS mese_accettazione,\n",
    "                    YEAR(g.accettazione_recapitista_con018_data) AS anno_accettazione,\n",
    "                    COUNT(g.requestid) AS totale_affidi\n",
    "                    FROM DF_GOLD g JOIN DF_CAP_REGIONE r ON (g.geokey = r.CAP)\n",
    "                    GROUP BY r.Regione, MONTH(g.accettazione_recapitista_con018_data), YEAR(g.accettazione_recapitista_con018_data) \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a986ce9a-cda2-4478-be20-82c7e03d3ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d34141-9109-4e52-a20c-3822fc8503ed",
   "metadata": {},
   "source": [
    "# Prova 2 esportare il risultato in tabella"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0202d2dc-037f-49ff-b3cf-eca77af9f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.createOrReplaceTempView(\"DF_OUTPUT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3944de0f-93ec-4514-9329-a2e89fa2ff50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    " spark.sql(\"\"\"SELECT * FROM DF_OUTPUT\"\"\").writeTo(\"send_dev.inps_territori\")\\\n",
    "                .using(\"iceberg\")\\\n",
    "                .tableProperty(\"format-version\",\"2\")\\\n",
    "                .tableProperty(\"engine.hive.enabled\",\"true\")\\\n",
    "                .createOrReplace()\n",
    "#print(datetime.now()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2107be96-382b-4e03-bfd9-bf0c880b6428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
